# 48H_Challenge_Bias_Detection

# ğŸ§  Bias Detection & Explainability in AI Hiring Models

âš ï¸âš ï¸ **Important Note on Dataset Format vs Challenge Context** âš ï¸âš ï¸
-- The dataset provided with the challenge was fully numerical and did not include any text fields (e.g., resume summaries or cover letters) as described in the challenge context. Therefore, we applied suitable tabular modeling techniques (Random Forest and XGBoost) and conducted fairness and explainability analysis accordingly, within the limits of the data format.

## ğŸš€ Challenge Overview
This project was built for the 48-hour challenge:  
**â€œUncovering Bias and Explaining Decisions in a Text-Based Job Screening Modelâ€**.

The task: simulate and audit a model that decides whether to hire applicants, and analyze if any **bias exists** (e.g., gender-based). The model is explainable via SHAP visualizations, and fairness metrics are reported.

---

## ğŸ“¦ Dataset Description
The dataset contains structured, numerical hiring-related features such as:

- `Age`, `ExperienceYears`, `SkillScore`, `PersonalityScore`, etc.
- `RecruitmentStrategy`: one-hot encoded
- `Gender`: Binary (`0 = Female`, `1 = Male`) â€” **used as the sensitive attribute**

---

## âš–ï¸ Handling Data Imbalance

- The dataset showed **class imbalance** between `Hire` and `Not Hire` classes.
- We applied:
  - `class_weight='balanced'` in RandomForest
  - `scale_pos_weight` in XGBoost based on training set ratios

---

## ğŸ§  Model Used

Two classifiers were trained and evaluated:

1. **RandomForestClassifier** (with class weight)
2. **XGBoostClassifier** (with scale_pos_weight)

Features were scaled (except for encoded/binary columns), and models were trained with **stratified split**.

---

## ğŸ“Š Model Performance

On the test set of 300 samples:

### ğŸ”¹ Random Forest
Confusion Matrix:
[[204 3]
[ 23 70]]

Accuracy: 91.3%
Macro F1 Score: 0.892
ROC AUC Score: 0.869

### ğŸ”¹ XGBoost
Confusion Matrix:
[[202 5]
[ 17 76]]

Accuracy: 92.7%
Macro F1 Score: 0.911
ROC AUC Score: 0.897 

---

## ğŸ§ª Fairness Analysis

The following **group fairness metrics** were evaluated by `Gender`:

| Metric             | Female (0) | Male (1)  |
|--------------------|------------|-----------|
| Selection Rate     | 22.2%      | 31.4%     |
| True Positive Rate | 75.0%      | 86.8%     |
| False Positive Rate| 1.9%       | 2.9%      |

ğŸ“‰ **Average Odds Difference** = `0.0639` â†’ Moderate disparity  
> AOD combines both FPR and TPR disparity between gender groups.

---

## ğŸ§  Explainability (SHAP)

We used **SHAP TreeExplainer** on the XGBoost model to visualize:

- ğŸ“Š Global Feature Importance
- ğŸ” Detailed dot summary plots for 5 predictions (3 Hire, 2 No-Hire)

Key findings:

- Features like `SkillScore`, `InterviewScore`, and `PersonalityScore` have high predictive weight.
- `Gender` had **low global SHAP impact**, but fairness metrics showed **outcome disparities**, suggesting potential **proxy bias** through other features.


